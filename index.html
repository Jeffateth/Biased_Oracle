<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Biased Oracle: Assessing LLMs' Understandability and Empathy in Medical Diagnoses</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap" rel="stylesheet">
</head>
<body>
    <section class="hero">
        <div class="container">
            <h1 class="main-title">The Biased Oracle: Assessing LLMs' Understandability and Empathy in Medical Diagnoses</h1>
            <div class="institution-logos">
                <img src="docs/assets/eth_logo_kurz_pos.svg" alt="ETH Zurich" class="logo">
                <img src="docs/assets/ETH_Health_Ethics_And_Policy_Lab_Logo.png" alt="Health Ethics & Policy Lab" class="logo">
                <img src="docs/assets/NeurIPS-logo.svg" alt="NeurIPS 2025" class="logo">
            </div>
            <div class="authors">
                <span class="author"><a href="https://www.linkedin.com/in/yaojia2025/">Jianzhou Yao</a><sup>1*</sup>,</span>
                <span class="author"><a href="https://www.linkedin.com/in/shunchang-liu-561393283/">Shunchang Liu</a><sup>2*</sup>,</span>
                <span class="author"><a href="https://www.linkedin.com/in/guillaume-drui-62728020a/">Guillaume Drui</a><sup>2</sup>,</span>
                <span class="author"><a href="https://www.linkedin.com/in/rikard-pettersson-633b45381/">Rikard Pettersson</a><sup>1</sup>,</span>
                <span class="author"><a href="https://hest.ethz.ch/en/department/people/people-a-z/personen-detail.MjQxMzQz.TGlzdC8zMzQsLTQ1MTk1NTQ5OA==.html">Alessandro Blasimme</a><sup>3</sup>,</span>
                <span class="author"><a href="https://hest.ethz.ch/en/department/people/people-a-z/personen-detail.MTY4MjQ4.TGlzdC8zMzQsLTQ1MTk1NTQ5OA==.html">Sara Kijewski</a><sup>3‚Ä†</sup></span>
            </div>
            
            <div class="affiliations">
                <span class="affiliation"><sup>1</sup> Dept. of Chemistry and Applied Biosciences, ETH Zurich</span>
                <span class="affiliation"><sup>2</sup> Dept. of Computer Science, ETH Zurich</span>
                <span class="affiliation"><sup>3</sup> Dept. of Health Sciences and Technology, ETH Zurich</span>
                <small class="contribution-note">
                    <sup>*</sup> Equal contribution &nbsp;&nbsp; <sup>‚Ä†</sup> Corresponding author
                </small>
            </div>
            <div class="links">
                <!-- Paper -->
                <a href="https://arxiv.org/abs/2511.00924" class="btn rounded">
                    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 16 16">
                        <path d="M4 0h8l4 4v12a2 2 0 0 1-2 2H4a2 2 0 0 1-2-2V2a2 2 0 0 1 2-2z"/>
                    </svg>
                    Paper
                </a>
                <!-- OpenReview -->
                <a href="https://openreview.net/forum?id=mhtDi2d4ZC&referrer=%5Bthe%20profile%20of%20Guillaume%20Drui%5D(%2Fprofile%3Fid%3D~Guillaume_Drui1)"
                   class="btn rounded"
                   target="_blank">
                    OpenReview
                </a>
                <!-- Code -->
                <a href="https://github.com/Jeffateth/Biased_Oracle" class="btn rounded" target="_blank">
                    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 16 16">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.19 0 .21.15.46.55.38A8.01 8.01 0 0 0 16 8c0-4.42-3.58-8-8-8z"/>
                    </svg>
                    Code
                </a>
                <!-- Workshop -->
                <a href="https://genai4health.github.io/" class="btn rounded" target="_blank">
                    <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" viewBox="0 0 16 16">
                        <path d="M3.5 0a.5.5 0 0 1 .5.5V1h8V.5a.5.5 0 0 1 1 0V1h1a2 2 0 0 1 2 2v1H0V3a2 2 0 0 1 2-2h1V.5a.5.5 0 0 1 .5-.5zM0 14a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V5H0v9z"/>
                    </svg>
                    Workshop
                </a>
                <!-- Cite -->
                <a href="#bibtex" class="btn rounded">
                    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 16 16">
                        <path d="M6 3h4v2H6V3zm-2 4h8v2H4V7zm2 4h4v2H6v-2z"/>
                    </svg>
                    Cite
                </a>
            </div>
        </div>
    </section>
    <section class="content-section">
        <div class="container">
            <h2>Abstract</h2>
            <div class="section-content">
                <p>Large language models (LLMs) show promise for supporting clinicians in diagnostic communication by generating explanations and guidance for patients. Yet their ability to produce outputs that are both understandable and empathetic remains uncertain. We evaluate two leading LLMs on medical diagnostic scenarios, assessing understandability using readability metrics as a proxy and empathy through LLM-as-a-Judge ratings compared to human evaluations.</p>
                <p>The results indicate that LLMs adapt explanations to socio-demographic variables and patient conditions. However, they also generate overly complex content and display biased affective empathy, leading to uneven accessibility and support. These patterns underscore the need for systematic calibration to ensure equitable patient communication.</p>
            </div>
        </div>
    </section>
    <section class="content-section" style="padding: 2rem 0;">
        <img src="framework_web.png" alt="Evaluation Framework" style="max-width: 85%; height: auto; margin: 0 auto; display: block;">
    </section>
    <section class="content-section gray-bg">
        <div class="container">
            <h2>Key Findings</h2>
            <div class="section-content">
                <div class="findings-grid">
                    <div class="finding-item">
                        <h4>üìö Overcomplexity Problem</h4>
                        <p>Both models generate text at 9th-13th grade reading level, well above the recommended 6th-8th grade for public health materials. This overcomplexity may reinforce health literacy disparities and limit accessibility for general patient populations.</p>
                    </div>
                    <div class="finding-item">
                        <h4>üß† Cognitive vs. Affective Empathy Split</h4>
                        <p>Cognitive empathy remains consistently high and stable (~2.8-3.0) across all demographic groups. In contrast, affective empathy shows substantial variation, shaped by medical diagnosis, patient education level, and the choice of evaluator model.</p>
                    </div>
                    <div class="finding-item">
                        <h4>üè• Diagnosis as Primary Driver</h4>
                        <p>Medical condition has the strongest and most consistent effect on affective empathy. Alzheimer's disease receives the highest empathy scores (~2.2-3.0), while chronic heart disease receives the lowest (~1.6-2.3).</p>
                    </div>
                    <div class="finding-item">
                        <h4>üéì Education Inverse Relationship</h4>
                        <p>Patients with medical degrees receive lower affective empathy than those with high school education. This suggests LLMs shift to a more technical, less emotionally expressive communication style when addressing medically trained individuals.</p>
                    </div>
                    <div class="finding-item">
                        <h4>‚öñÔ∏è Systematic Self-Evaluation Bias</h4>
                        <p>GPT inflates its own affective empathy scores by +0.333 points, while Claude deflates its own scores by -0.256 points. These self-evaluation biases hold consistently across all demographic groups, revealing systematic patterns in model self-assessment.</p>
                    </div>
                    <div class="finding-item">
                        <h4>üë• Overconfidence vs. Human Judges</h4>
                        <p>GPT rates its own outputs significantly higher than human annotators do (all p < 0.05). Critically, LLMs fail to detect demographic biases that human evaluators identify, such as lower empathy toward African females compared to European female in chosen gpt responses. </p>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <section class="content-section">
        <div class="container">
            <h3>Scenario Design</h3>
            <p>Our evaluation framework encompasses 156 diagnostic scenarios that systematically vary across multiple dimensions:</p>
            <ul>
                <li><strong>4 Medical Conditions:</strong> Obesity, Pancreatic cancer, Alzheimer's disease, Chronic ischemic heart disease</li>
                <li><strong>3 Education Levels:</strong> High school diploma or lower, University degree, Medical degree</li>
                <li><strong>4 Age Groups (analysis):</strong> &lt;18, 18‚Äì49, 50‚Äì64, 65+</li>
                <li><strong>2 Genders:</strong> Male and female</li>
                <li><strong>3 Geographic Groups:</strong> European, African, Asian</li>
            </ul>
            <h3>Evaluation Methodology</h3>
            <p>We assess model outputs using two complementary approaches:</p>
            <ul>
                <li><strong>Understandability:</strong> Measured using 5 standard readability metrics including Flesch-Kincaid Grade Level, SMOG Index, Gunning Fog Index, Coleman-Liau Index, and Dale-Chall Readability Score</li>
                <li><strong>Empathy:</strong> Evaluated through both affective (emotional resonance) and cognitive (perspective-taking) dimensions using LLM-as-a-judge approaches validated against human ratings</li>
            </ul>
            <h3>Broader Impacts</h3>
            <p>Our study reveals that LLMs, if deployed in medical contexts without careful safeguards, risk amplifying existing health inequities through excessive complexity and biased empathy responses. Transparent, bias-aware evaluation is critical before any clinical integration.</p>
            <p><strong>Important Note:</strong> This study evaluates LLM-generated synthetic diagnostic communications as an exploratory framework for investigating potential biases. It does not endorse the use of LLMs in actual clinical settings. All scenarios are synthetic; no real patient information is used.</p>
        </div>
    </section>
    <section id="bibtex" class="content-section">
        <div class="container">
            <h2>BibTeX</h2>
            <div class="section-content">
                <pre><code>@inproceedings{
    yao2025the,
    title={The Biased Oracle: Assessing {LLM}s{\textquoteright} Understandability and Empathy in Medical Diagnoses},
    author={Jianzhou Yao and Shunchang Liu and Guillaume Drui and Rikard Pettersson and Alessandro Blasimme and Sara Kijewski},
    booktitle={The Second Workshop on GenAI for Health: Potential, Trust, and Policy Compliance},
    year={2025},
    url={https://openreview.net/forum?id=mhtDi2d4ZC}
    }
}</code></pre>
            </div>
        </div>
    </section>
    <footer>
        <div class="container">
            <p>Official repository for our NeurIPS 2025 GenAI4Health workshop paper</p>
            <p>All data are synthetic; no real patient information is used.</p>
            <p class="license">CC BY 4.0 ‚Äì Free to use with attribution</p>
            <p class="copyright">¬© 2025 ETH Z√ºrich ¬∑ Health Ethics & Policy Lab ¬∑ NeurIPS Foundation</p>
        </div>
    </footer>
</body>
</html>
