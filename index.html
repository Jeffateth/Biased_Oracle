<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Biased Oracle: Assessing LLMs' Understandability and Empathy in Medical Diagnoses</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap" rel="stylesheet">
</head>
<body>
    <section class="hero">
        <div class="container">
            <h1 class="main-title">The Biased Oracle: Assessing LLMs' Understandability and Empathy in Medical Diagnoses</h1>
            <div class="institution-logos">
                <img src="docs/assets/eth_logo_kurz_pos.svg" alt="ETH Zurich" class="logo">
                <img src="docs/assets/ETH_Health_Ethics_And_Policy_Lab_Logo.png" alt="Health Ethics & Policy Lab" class="logo">
                <img src="docs/assets/NeurIPS-logo.svg" alt="NeurIPS 2025" class="logo">
            </div>
            <div class="authors">
                <span class="author"><a href="#">Jianzhou Yao</a><sup>1‚Ä†</sup>,</span>
                <span class="author"><a href="#">Shunchang Liu</a><sup>2‚Ä†</sup>,</span>
                <span class="author"><a href="#">Guillaume Drui</a><sup>2</sup>,</span>
                <span class="author"><a href="#">Rikard Pettersson</a><sup>1</sup>,</span>
                <span class="author"><a href="#">Alessandro Blasimme</a><sup>3</sup>,</span>
                <span class="author"><a href="mailto:sara.kijewski@hest.ethz.ch">Sara Kijewski</a><sup>3*</sup></span>
            </div>
            <div class="affiliations">
                <span class="affiliation"><sup>1</sup>Dept. of Chemistry and Applied Biosciences, ETH Zurich</span>
                <span class="affiliation"><sup>2</sup>Dept. of Computer Science, ETH Zurich</span>
                <span class="affiliation"><sup>3</sup>Dept. of Health Sciences and Technology, ETH Zurich</span>
                <small class="contribution-note"><sup>‚Ä†</sup>Equal contribution <sup>*</sup>Corresponding author</small>
            </div>
            <div class="links">
                <a href="https://arxiv.org/abs/2511.00924" class="btn rounded"><span>üìù</span> Paper</a>
                <a href="#" class="btn rounded"><span>üóÑÔ∏è</span> Dataset</a>
                <a href="https://github.com/Jeffateth/Biased_Oracle" class="btn rounded"><span>üêô</span> Code</a>
                <a href="#bibtex" class="btn rounded"><span>‚ùù‚ùû</span> Cite</a>
            </div>
        </div>
    </section>
    <section class="content-section">
        <div class="container">
            <h2>Abstract</h2>
            <div class="section-content">
                <p>We evaluate GPT-4o and Claude-3.7 on 156 synthetic medical diagnostic scenarios across diverse patient demographics, assessing understandability using 5 standard readability metrics (Flesch-Kincaid, SMOG, Gunning Fog, Coleman-Liau, Dale-Chall) and empathy through affective and cognitive dimensions via LLM-as-a-judge and human ratings.</p>
                <p>Our study examines 156 diagnostic scenarios combining 4 conditions √ó 3 education levels √ó 4 age groups √ó 2 genders √ó 3 regions. We find that texts exceed 6th‚Äì8th grade readability standards for health materials, and identify consistent empathy bias by condition, age, and education. Additionally, we uncover evaluator bias in LLM-as-a-judge settings, highlighting critical limitations in current evaluation methodologies.</p>
            </div>
        </div>
    </section>
    <section class="content-section gray-bg">
        <div class="container">
            <h2>Key Findings</h2>
            <div class="section-content">
                <div class="findings-grid">
                    <div class="finding-item">
                        <h4>üìä Diagnosis Dominates Empathy</h4>
                        <p>Alzheimer's disease receives the highest empathy scores, while heart disease receives the lowest. The type of diagnosis significantly influences the empathetic tone of LLM-generated explanations, revealing systematic biases in how models respond to different medical conditions.</p>
                    </div>
                    <div class="finding-item">
                        <h4>üéì Education Bias</h4>
                        <p>Patients with medical degrees receive lower affective empathy in diagnostic explanations. This suggests that LLMs adjust their emotional tone based on perceived patient expertise, potentially underserving highly educated patients' emotional needs.</p>
                    </div>
                    <div class="finding-item">
                        <h4>üë• Age Effects are Rater-Dependent</h4>
                        <p>A U-shaped relationship between patient age and empathy is visible with some evaluators but not others. This inconsistency highlights the subjective nature of empathy assessment and the importance of evaluator selection in LLM evaluation frameworks.</p>
                    </div>
                    <div class="finding-item">
                        <h4>‚ö†Ô∏è LLM-as-Judge Disagreement</h4>
                        <p>Poor inter-rater reliability between different LLM evaluators reveals fundamental challenges in using LLMs to assess subjective qualities like empathy. This finding calls into question the reliability of LLM-as-a-judge methodologies for evaluating nuanced human-centric attributes.</p>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <section class="content-section">
        <div class="container">
            <h2>Overview</h2>
            <div class="section-content">
                <p>Large language models have shown remarkable capabilities in generating medical explanations, but their ability to maintain understandability and empathy across diverse patient demographics remains underexplored. This work presents a comprehensive evaluation of two state-of-the-art models, GPT-4o and Claude-3.7, across 156 carefully designed synthetic medical diagnostic scenarios.</p>
                <h3>Scenario Design</h3>
                <p>Our evaluation framework encompasses 156 diagnostic scenarios that systematically vary across multiple dimensions:</p>
                <ul>
                    <li><strong>4 Medical Conditions:</strong> Alzheimer's disease, heart disease, diabetes, and cancer</li>
                    <li><strong>3 Education Levels:</strong> High school, bachelor's degree, and medical degree</li>
                    <li><strong>4 Age Groups:</strong> Young adults, middle-aged, seniors, and elderly</li>
                    <li><strong>2 Genders:</strong> Male and female</li>
                    <li><strong>3 Geographic Regions:</strong> North America, Europe, and Asia</li>
                </ul>
                <h3>Evaluation Methodology</h3>
                <p>We assess model outputs using two complementary approaches:</p>
                <ul>
                    <li><strong>Understandability:</strong> Measured using 5 standard readability metrics including Flesch-Kincaid Grade Level, SMOG Index, Gunning Fog Index, Coleman-Liau Index, and Dale-Chall Readability Score</li>
                    <li><strong>Empathy:</strong> Evaluated through both affective (emotional warmth) and cognitive (perspective-taking) dimensions using LLM-as-a-judge approaches validated against human ratings</li>
                </ul>
            </div>
        </div>
    </section>
    <section class="content-section gray-bg">
        <div class="container">
            <h2>Quick Start</h2>
            <div class="section-content">
                <pre><code># Clone and setup
git clone https://github.com/Jeffateth/Biased_Oracle
cd Biased_Oracle
conda env create -f environment.yml
conda activate AI4Good

# Generate figures
python src/readability_metrics_plot.py
python src/plot_empathy.py</code></pre>
                <p><strong>Note:</strong> Precomputed results are included in the repository. Regenerating outputs requires OpenAI and Anthropic API keys.</p>
                <h3>Repository Structure</h3>
                <pre><code>Biased_Oracle
‚îú‚îÄ‚îÄ data/          # Prompts, model outputs, ratings
‚îú‚îÄ‚îÄ notebooks/     # Data collection & scoring
‚îú‚îÄ‚îÄ src/           # Analysis & plotting scripts
‚îî‚îÄ‚îÄ outputs/       # Figures and statistical results</code></pre>
            </div>
        </div>
    </section>
    <section class="content-section">
        <div class="container">
            <h2>BibTeX</h2>
            <div class="section-content">
                <pre><code>@misc{yao2025biasedoracleassessingllms,
    title={The Biased Oracle: Assessing LLMs' Understandability and Empathy in Medical Diagnoses}, 
    author={Jianzhou Yao and Shunchang Liu and Guillaume Drui and Rikard Pettersson and Alessandro Blasimme and Sara Kijewski},
    year={2025},
    eprint={2511.00924},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2511.00924}, 
}</code></pre>
            </div>
        </div>
    </section>
    <footer>
        <div class="container">
            <p>Official repository for our NeurIPS 2025 GenAI4Health workshop paper</p>
            <p>All data are synthetic; no real patient information is used.</p>
            <p class="license">CC BY 4.0 ‚Äì Free to use with attribution</p>
            <p class="copyright">¬© 2025 ETH Z√ºrich ¬∑ Health Ethics & Policy Lab ¬∑ NeurIPS Foundation</p>
        </div>
    </footer>
</body>
</html>
